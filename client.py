# client.py
import os
import asyncio
import httpx
from dotenv import load_dotenv
from dataclasses import dataclass, field
from typing import Any
from langchain.schema import HumanMessage, SystemMessage

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from gen_ai_hub import GenAIHubProxyClient

load_dotenv()


# Create server parameters
server_params = StdioServerParameters(
    command="python",
    args=["./server.py"],
    env=None,
)

async def get_token():
    async with httpx.AsyncClient() as client:
        resp = await client.post(
            os.environ['AICORE_AUTH_URL'] + "/oauth/token",
            data={"grant_type": "client_credentials"},
            auth=(os.environ['AICORE_CLIENT_ID'], os.environ['AICORE_CLIENT_SECRET']),
        )
        resp.raise_for_status()
        return resp.json()["access_token"]


async def call_claude(prompt):
    token = await get_token()
    LLM_DEPLOYMENT_ID = os.environ.get("LLM_DEPLOYMENT_ID")
    url = f"{os.environ['AICORE_BASE_URL']}/deployments/{LLM_DEPLOYMENT_ID}/inference"
    headers = {
        "Authorization": f"Bearer {token}",
        "AI-Resource-Group": os.environ['AICORE_RESOURCE_GROUP'],
        "Content-Type": "application/json"
    }
    payload = {"prompt": prompt}
    async with httpx.AsyncClient() as client:
        resp = await client.post(url, headers=headers, json=payload)
        resp.raise_for_status()
        return resp.json()


@dataclass
class Chat:
    messages: list[Any] = field(default_factory=list)

    system_prompt: str = """You are a master SQL assistant connected to SAP HANA Cloud.
You help users run queries and return results."""

    async def process_query(self, session: ClientSession, query: str) -> None:
        self.messages.append(HumanMessage(content=query))

        response = await session.list_tools()
        tool_names = [tool.name for tool in response.tools]

        if "query_data" in tool_names:
            # Ask LLM what SQL to run
            full_prompt = [
                SystemMessage(content=self.system_prompt),
                *self.messages
            ]

            llm = ClaudeLLM(os.environ.get("LLM_DEPLOYMENT_ID"))
            llm_response = await llm.ainvoke(full_prompt)
            sql = llm_response.content.strip()

            print(f"\nâž¡ SQL generated by LLM:\n{sql}")

            # Call tool with SQL
            result = await session.call_tool("query_data", {"sql": sql})
            result_text = getattr(result.content[0], "text", "")

            print("\nðŸŸ¢ Query Result:")
            print(result_text)

        else:
            print("No tool named 'query_data' found!")

    async def chat_loop(self, session: ClientSession):
        while True:
            query = input("\nðŸ’¬ Your Query: ").strip()
            if query.lower() in {"exit", "quit"}:
                break
            await self.process_query(session, query)

    async def run(self):
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                await self.chat_loop(session)
async def main():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            while True:
                query = input("\nQuery: ").strip()
                # Call Claude via SAP AI Core
                llm_response = await call_claude(query)
                print("Claude response:", llm_response)
                # Optionally, you can also call your tool via session.call_tool if needed

class ClaudeLLM:
    def __init__(self, deployment_id):
        self.deployment_id = deployment_id

    async def ainvoke(self, prompt):
        token = await get_token()
        url = f"{os.environ['AICORE_BASE_URL']}/deployments/{self.deployment_id}/inference"
        headers = {
            "Authorization": f"Bearer {token}",
            "AI-Resource-Group": os.environ['AICORE_RESOURCE_GROUP'],
            "Content-Type": "application/json"
        }
        payload = {"prompt": prompt}
        async with httpx.AsyncClient() as client:
            resp = await client.post(url, headers=headers, json=payload)
            resp.raise_for_status()
            return resp.json()

# Usage:
# Move this usage example inside an async function if you want to run it.
# For example, you can add it to your main() function or another async function.

if __name__ == "__main__":
    asyncio.run(main())
